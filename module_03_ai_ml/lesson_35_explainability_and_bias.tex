\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx,booktabs,adjustbox,multicol,amsmath,amssymb}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255,127,14}
\definecolor{mlgreen}{RGB}{44,160,44}
\definecolor{mlred}{RGB}{214,39,40}
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Bottom note command for key takeaways
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}
\title{Digital Finance 3: Technology in Finance}
\subtitle{Lesson 35: Explainability and Bias}
\author{FHGR}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\bottomnote{Explainability builds trust and enables regulatory compliance.}
\end{frame}

\begin{frame}[t]{Learning Objectives}
By the end of this lesson, you will be able to:
\begin{itemize}
\item Explain the interpretability-accuracy trade-off
\item Apply SHAP and LIME for model explanations
\item Understand feature attribution methods
\item Detect and mitigate algorithmic bias
\item Evaluate fairness metrics in financial ML
\item Navigate regulatory requirements (GDPR Article 22)
\end{itemize}
\bottomnote{LIME provides local explanations by approximating complex models with simple ones.}
\end{frame}

\begin{frame}[t]{SHAP Values for Feature Importance}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/shap_values/shap_values.pdf}
\end{center}
\bottomnote{SHAP values decompose predictions into individual feature contributions based on game theory.}
\end{frame}

\begin{frame}[t]{LIME: Local Interpretable Model-Agnostic Explanations}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/lime_explanation/lime_explanation.pdf}
\end{center}
\bottomnote{LIME approximates black-box models locally with interpretable linear models for individual predictions.}
\end{frame}

\begin{frame}[t]{Feature Attribution Methods Comparison}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/feature_attribution/feature_attribution.pdf}
\end{center}
\bottomnote{Different attribution methods provide complementary insights into model behavior and feature importance.}
\end{frame}

\begin{frame}[t]{Partial Dependence and ICE Plots}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/pdp_ice_plots/pdp_ice_plots.pdf}
\end{center}
\bottomnote{PDP shows average marginal effects; ICE plots reveal heterogeneous effects across instances.}
\end{frame}

\begin{frame}[t]{Model-Agnostic Explainability Methods}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/model_agnostic_methods/model_agnostic_methods.pdf}
\end{center}
\bottomnote{Model-agnostic methods work with any ML model, enabling consistent explanations across model types.}
\end{frame}

\begin{frame}[t]{Algorithmic Bias Sources}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/algorithmic_bias/algorithmic_bias.pdf}
\end{center}
\bottomnote{Bias can arise from training data, feature selection, model design, or deployment decisions.}
\end{frame}

\begin{frame}[t]{Fairness Metrics Comparison}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/fairness_metrics/fairness_metrics.pdf}
\end{center}
\bottomnote{Multiple fairness definitions exist; choosing the right metric depends on context and stakeholder values.}
\end{frame}

\begin{frame}[t]{Summary}
\textbf{Key Takeaways:}
\begin{itemize}
\item Explainability required by regulations (GDPR Article 22)
\item SHAP and LIME most popular explanation methods
\item Trade-off: accuracy vs. interpretability
\item Bias detection critical for fair lending and hiring
\item Multiple fairness metrics (no one-size-fits-all)
\item Explainability tools maturing rapidly
\end{itemize}

\vspace{1em}
\textbf{Next Lesson:} AI Regulation and Future
\bottomnote{Feature importance in credit models must be explainable to regulators.}
\end{frame}

\end{document}
