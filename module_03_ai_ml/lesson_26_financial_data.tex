\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx,booktabs,adjustbox,multicol,amsmath,amssymb}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255,127,14}
\definecolor{mlgreen}{RGB}{44,160,44}
\definecolor{mlred}{RGB}{214,39,40}
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Bottom note command for key takeaways
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}
\title{Digital Finance 3: Technology in Finance}
\subtitle{Lesson 26: Financial Data for AI/ML}
\author{FHGR}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\bottomnote{Data quality determines model quality - garbage in, garbage out.}
\end{frame}

\begin{frame}[t]{Learning Objectives}
By the end of this lesson, you will be able to:
\begin{itemize}
\item Distinguish between structured and unstructured financial data
\item Identify major sources and vendors of financial data
\item Understand the alternative data revolution and its applications
\item Recognize data quality issues and preprocessing requirements
\item Explain GDPR and privacy implications for financial ML
\item Describe basic feature engineering concepts
\end{itemize}
\bottomnote{Domain expertise in feature engineering often matters more than model complexity.}
\end{frame}

\begin{frame}[t]{Structured vs. Unstructured Data}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/financial_data_types/financial_data_types.pdf}
\end{center}
\vspace{0.5em}
\textbf{Modern ML:} Combines both types (e.g., sentiment scores from text as features in tabular models).
\bottomnote{Understanding different data types is essential for effective ML applications.}
\end{frame}

\begin{frame}[t]{Traditional Financial Data Sources}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Market Data:}
\begin{itemize}
\item Stock prices (exchanges)
\item Bond yields (TRACE, Bloomberg)
\item Derivatives (CME, Eurex)
\item FX rates (interbank, EBS)
\item Crypto (Coinbase, Binance)
\end{itemize}

\vspace{0.5em}
\textbf{Fundamental Data:}
\begin{itemize}
\item Financial statements (EDGAR, SEDAR)
\item Company events (earnings, M\&A)
\item Economic indicators (BLS, Fed, ECB)
\item Industry metrics (PMI, CPI)
\end{itemize}

\column{0.48\textwidth}
\textbf{Credit/Risk Data:}
\begin{itemize}
\item Credit bureaus (Experian, Equifax, TransUnion)
\item Ratings (Moody's, S\&P, Fitch)
\item Loan performance data
\item Default histories
\end{itemize}

\vspace{0.5em}
\textbf{Major Vendors:}
\begin{itemize}
\item Bloomberg Terminal (\$20-25k/year)
\item Refinitiv (formerly Thomson Reuters)
\item FactSet
\item S\&P Capital IQ
\item Morningstar
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Trend:} Declining costs for basic data (Yahoo Finance free), but premium data remains expensive.
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{The Alternative Data Revolution}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What is Alternative Data?}
\begin{itemize}
\item Non-traditional data sources
\item Often unstructured or semi-structured
\item Provides early signals
\item Competitive edge (information advantage)
\end{itemize}

\vspace{0.5em}
\textbf{Categories:}
\begin{enumerate}
\item \textbf{Web-scraped:} Prices, reviews, job postings
\item \textbf{Sensor/IoT:} Satellite, credit cards, mobile location
\item \textbf{Social:} Twitter sentiment, Reddit mentions
\item \textbf{Business:} Email receipts, app usage
\end{enumerate}

\column{0.48\textwidth}
\textbf{Example Use Cases:}
\begin{itemize}
\item Satellite images: Count cars in parking lots (retail sales proxy)
\item Credit card data: Real-time consumer spending
\item Job postings: Company growth indicators
\item App downloads: User engagement trends
\item Shipping data: Supply chain analysis
\end{itemize}

\vspace{0.5em}
\textbf{Market Size:}
\begin{itemize}
\item \$1.7B in 2020
\item Projected \$17B by 2027
\item Hedge funds are largest buyers
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Challenges:} Quality control, legal/ethical concerns, data decay (alpha decay).
\bottomnote{Financial data includes structured (prices), semi-structured (news), and unstructured (social media).}
\end{frame}

\begin{frame}[t]{Alternative Data: Examples in Detail}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Satellite Imagery:}
\begin{itemize}
\item Providers: Orbital Insight, RS Metrics
\item Use: Count oil tanks, construction activity
\item Example: China steel production estimates
\item Frequency: Daily to weekly
\item Cost: \$10k-100k+ per year
\end{itemize}

\vspace{0.5em}
\textbf{Credit Card Transactions:}
\begin{itemize}
\item Providers: Facteus, Second Measure
\item Use: Real-time revenue tracking
\item Example: Restaurant chain performance
\item Privacy: Aggregated, anonymized
\end{itemize}

\column{0.48\textwidth}
\textbf{Social Media Sentiment:}
\begin{itemize}
\item Providers: RavenPack, Bloomberg sentiment
\item Use: Market mood, event detection
\item Example: Tweet volume predicting volatility
\item Challenges: Noise, manipulation
\end{itemize}

\vspace{0.5em}
\textbf{Web Traffic:}
\begin{itemize}
\item Providers: SimilarWeb, Alexa (discontinued)
\item Use: Company engagement metrics
\item Example: E-commerce site visits
\item Limitation: Sample-based estimates
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Key Question:} Does alternative data provide genuine alpha or just noise?
Evidence: Mixed, diminishing returns as adoption increases (alpha decay).
\bottomnote{Case studies provide concrete evidence of technology impact and adoption patterns.}
\end{frame}

\begin{frame}[t]{Data Quality: The GIGO Principle}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Garbage In, Garbage Out:}
\begin{itemize}
\item ML models amplify data quality issues
\item No algorithm fixes bad data
\item Quality > Quantity (usually)
\end{itemize}

\vspace{0.5em}
\textbf{Common Data Problems:}
\begin{itemize}
\item \textbf{Missing values:} Deletions, NaN, nulls
\item \textbf{Outliers:} Errors vs. true extremes
\item \textbf{Inconsistencies:} Units, formats, definitions
\item \textbf{Duplicates:} Same record multiple times
\item \textbf{Errors:} Typos, wrong values
\end{itemize}

\column{0.48\textwidth}
\textbf{Finance-Specific Issues:}
\begin{itemize}
\item \textbf{Survivorship bias:} Only successful firms remain
\item \textbf{Look-ahead bias:} Using future information
\item \textbf{Corporate actions:} Splits, dividends, mergers
\item \textbf{Restatements:} Accounting changes, revisions
\item \textbf{Stale data:} Delayed or infrequent updates
\end{itemize}

\vspace{0.5em}
\textbf{Impact on ML:}
\begin{itemize}
\item Biased predictions
\item Overfitting to noise
\item Poor generalization
\item Misleading performance metrics
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Best Practice:} Spend 50-80\% of project time on data cleaning and validation.
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{Data Preprocessing Pipeline}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/data_preprocessing_pipeline/data_preprocessing_pipeline.pdf}
\end{center}
\vspace{0.5em}
\textbf{Automation:} Modern ML pipelines use tools like Apache Airflow, Prefect for orchestration.
\bottomnote{Systematic preprocessing is critical for model quality and reproducibility.}
\end{frame}

\begin{frame}[t]{Handling Missing Data}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Why Data is Missing:}
\begin{enumerate}
\item \textbf{MCAR} (Missing Completely At Random): Pure chance, no pattern
\item \textbf{MAR} (Missing At Random): Related to observed data
\item \textbf{MNAR} (Missing Not At Random): Related to unobserved value itself
\end{enumerate}

\vspace{0.5em}
\textbf{Finance Example:}
\begin{itemize}
\item MCAR: Random system glitch
\item MAR: Small firms don't report segment data
\item MNAR: Firms hide bad performance
\end{itemize}

\column{0.48\textwidth}
\textbf{Strategies:}
\begin{itemize}
\item \textbf{Deletion:} Drop rows/columns (only if < 5\% missing, MCAR)
\item \textbf{Mean/Median imputation:} Replace with average (simple, biased)
\item \textbf{Forward/backward fill:} Time series (assumes persistence)
\item \textbf{Model-based:} Predict missing values (KNN, regression)
\item \textbf{Indicator variable:} Flag missingness as feature
\end{itemize}

\vspace{0.5em}
\textbf{Best Practice:}
\begin{itemize}
\item Understand WHY data is missing
\item Test sensitivity to imputation method
\item Document assumptions
\end{itemize}
\end{columns}
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{Feature Engineering Fundamentals}
\begin{center}
\includegraphics[width=0.60\textwidth]{figures/feature_engineering_finance/feature_engineering_finance.pdf}
\end{center}
\vspace{0.5em}
\textbf{Art + Science:} Combines domain expertise with systematic experimentation to create predictive features.
\bottomnote{Feature engineering often has greater impact than algorithm selection.}
\end{frame}

\begin{frame}[t]{GDPR and Data Privacy Regulations}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{GDPR Key Principles (EU, 2018):}
\begin{itemize}
\item \textbf{Lawfulness:} Legal basis for processing
\item \textbf{Purpose limitation:} Specific, explicit purposes
\item \textbf{Data minimization:} Collect only necessary data
\item \textbf{Accuracy:} Keep data up-to-date
\item \textbf{Storage limitation:} Retain only as long as needed
\item \textbf{Integrity/confidentiality:} Secure processing
\end{itemize}

\vspace{0.5em}
\textbf{Individual Rights:}
\begin{itemize}
\item Right to access
\item Right to erasure (``right to be forgotten'')
\item Right to explanation (Article 22)
\end{itemize}

\column{0.48\textwidth}
\textbf{Implications for ML:}
\begin{itemize}
\item Consent requirements (explicit for sensitive data)
\item Anonymization challenges (re-identification risk)
\item Model explainability (if automated decision-making)
\item Data retention policies
\item Cross-border data transfers (adequacy decisions)
\end{itemize}

\vspace{0.5em}
\textbf{Other Regulations:}
\begin{itemize}
\item CCPA (California Consumer Privacy Act)
\item LGPD (Brazil)
\item POPIA (South Africa)
\end{itemize}

\vspace{0.5em}
\textbf{Penalties:} Up to 4\% of global revenue or 20M EUR (whichever is higher).
\end{columns}
\bottomnote{Regulatory frameworks shape adoption patterns and industry structure.}
\end{frame}

\begin{frame}[t]{Anonymization vs. Pseudonymization}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Anonymization:}
\begin{itemize}
\item Irreversible removal of identifiers
\item No longer personal data under GDPR
\item Techniques:
  \begin{itemize}
  \item Aggregation
  \item Noise addition (differential privacy)
  \item Generalization (age â†’ age range)
  \end{itemize}
\item Challenge: Re-identification risk (AOL search data, Netflix prize)
\end{itemize}

\vspace{0.5em}
\textbf{Re-identification Example:}
87\% of US population identifiable with just:
\begin{itemize}
\item ZIP code
\item Gender
\item Date of birth
\end{itemize}

\column{0.48\textwidth}
\textbf{Pseudonymization:}
\begin{itemize}
\item Replace identifiers with pseudonyms (tokens)
\item Reversible with key
\item Still personal data under GDPR
\item Techniques:
  \begin{itemize}
  \item Hashing
  \item Encryption
  \item Tokenization
  \end{itemize}
\item Reduces risk but doesn't eliminate it
\end{itemize}

\vspace{0.5em}
\textbf{Finance Use Cases:}
\begin{itemize}
\item Credit scoring: Pseudonymized customer IDs
\item Fraud detection: Anonymized transaction patterns
\item AML: Must balance privacy with compliance
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Best Practice:} Privacy by design (build privacy into system architecture from the start).
\bottomnote{Comparative analysis helps identify the right tool for specific requirements.}
\end{frame}

\begin{frame}[t]{Data Bias: Types and Examples}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Selection Bias:}
\begin{itemize}
\item Sample not representative of population
\item Example: Credit model trained only on approved loans (missing rejected applicants who would have repaid)
\end{itemize}

\vspace{0.3em}
\textbf{Survivorship Bias:}
\begin{itemize}
\item Only successful entities remain in dataset
\item Example: Mutual fund returns (failed funds disappear)
\item Impact: Overstates historical performance
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/survivorship_bias/survivorship_bias.pdf}
\end{center}
\end{columns}

\vspace{0.5em}
\textbf{Key Insight:} Bias in data leads to biased models, which lead to unfair outcomes. Proactive detection is essential.
\bottomnote{Data preprocessing typically consumes 80% of ML project time in finance.}
\end{frame}

\begin{frame}[t]{Time Series Data: Special Considerations}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Unique Challenges:}
\begin{itemize}
\item \textbf{Temporal dependence:} Observations not independent
\item \textbf{Non-stationarity:} Statistical properties change over time
\item \textbf{Seasonality:} Recurring patterns (quarterly earnings)
\item \textbf{Trends:} Long-term drift
\item \textbf{Structural breaks:} Regime changes (crises)
\end{itemize}

\vspace{0.5em}
\textbf{Cannot Use Standard ML:}
\begin{itemize}
\item Random train/test split violates temporal order
\item Cross-validation needs time-aware folds
\item Risk of look-ahead bias
\end{itemize}

\column{0.48\textwidth}
\textbf{Proper Approach:}
\begin{itemize}
\item \textbf{Walk-forward validation:} Train on past, test on future
\item \textbf{Expanding window:} Grow training set over time
\item \textbf{Rolling window:} Fixed-size window (adapts to recent data)
\end{itemize}

\vspace{0.5em}
\textbf{Feature Engineering:}
\begin{itemize}
\item Lags (t-1, t-2, ..., t-n)
\item Rolling statistics (moving averages, volatility)
\item Date/time features (day of week, month, quarter)
\item Event indicators (earnings announcement, FOMC)
\end{itemize}

\vspace{0.5em}
\textbf{Stationarity Tests:}
\begin{itemize}
\item Augmented Dickey-Fuller (ADF)
\item Differencing if non-stationary
\end{itemize}
\end{columns}
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{Data Storage and Infrastructure}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Storage Options:}
\begin{itemize}
\item \textbf{Relational databases:} PostgreSQL, MySQL (structured data, ACID transactions)
\item \textbf{NoSQL:} MongoDB, Cassandra (unstructured, scale)
\item \textbf{Data warehouses:} Snowflake, Redshift (analytics)
\item \textbf{Data lakes:} S3, Azure Data Lake (raw data, all formats)
\item \textbf{Time series DB:} InfluxDB, TimescaleDB (high-frequency data)
\end{itemize}

\vspace{0.5em}
\textbf{File Formats:}
\begin{itemize}
\item CSV (simple, inefficient)
\item Parquet (columnar, compressed, fast)
\item HDF5 (hierarchical, scientific)
\end{itemize}

\column{0.48\textwidth}
\textbf{Cloud vs. On-Premise:}
\begin{itemize}
\item Cloud: Scalability, cost-effective (pay-as-you-go)
\item On-prem: Control, security (for sensitive data)
\item Hybrid: Regulatory compliance + flexibility
\end{itemize}

\vspace{0.5em}
\textbf{Data Governance:}
\begin{itemize}
\item Data catalog (metadata management)
\item Access control (role-based)
\item Audit logs (who accessed what, when)
\item Data quality monitoring
\item Lineage tracking (source to destination)
\end{itemize}

\vspace{0.5em}
\textbf{Cost Considerations:}
\begin{itemize}
\item Storage: Cheap (S3 \$0.023/GB/month)
\item Compute: Expensive (processing, queries)
\item Data transfer: Can be costly (egress fees)
\end{itemize}
\end{columns}
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{Real-Time vs. Batch Data Processing}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Batch Processing:}
\begin{itemize}
\item Process large volumes periodically
\item Daily, weekly, monthly updates
\item Use cases:
  \begin{itemize}
  \item Monthly credit score updates
  \item Quarterly portfolio rebalancing
  \item Annual financial statement analysis
  \end{itemize}
\item Tools: Apache Spark, Hadoop, SQL
\item Pro: Efficient for large datasets
\item Con: Latency (hours to days)
\end{itemize}

\column{0.48\textwidth}
\textbf{Real-Time (Streaming):}
\begin{itemize}
\item Process data as it arrives
\item Milliseconds to seconds latency
\item Use cases:
  \begin{itemize}
  \item Fraud detection (transaction monitoring)
  \item Algorithmic trading (tick data)
  \item AML alerts (pattern matching)
  \end{itemize}
\item Tools: Apache Kafka, Flink, Kinesis
\item Pro: Immediate insights
\item Con: Complex infrastructure, costly
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Lambda Architecture:} Hybrid approach combining batch (accuracy) and streaming (speed) layers.
\bottomnote{Understanding the process flow is key to identifying optimization opportunities.}
\end{frame}

\begin{frame}[t]{Data Vendors: Comparison}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Bloomberg Terminal:}
\begin{itemize}
\item Cost: \$20-25k/user/year
\item Coverage: 99\% of financial instruments
\item Strengths: Real-time, analytics, news, messaging
\item Weaknesses: Expensive, proprietary
\end{itemize}

\vspace{0.3em}
\textbf{Refinitiv (LSEG):}
\begin{itemize}
\item Cost: \$15-30k/year
\item Strengths: Historical data, fundamentals
\item Eikon platform (Excel integration)
\end{itemize}

\vspace{0.3em}
\textbf{FactSet:}
\begin{itemize}
\item Cost: \$10-15k/year
\item Strengths: Quantitative analytics, screening
\item Popular among asset managers
\end{itemize}

\column{0.48\textwidth}
\textbf{Free/Low-Cost Alternatives:}
\begin{itemize}
\item Yahoo Finance: Free (15-20 min delay)
\item Alpha Vantage: API, free tier (500 calls/day)
\item Quandl: Free + premium datasets
\item FRED (Federal Reserve): Economic data
\end{itemize}

\vspace{0.3em}
\textbf{Alternative Data:}
\begin{itemize}
\item Thinknum (\$10k-50k/year)
\item YipitData (\$50k+/year)
\item Eagle Alpha (marketplace)
\end{itemize}

\vspace{0.3em}
\textbf{Selection Criteria:}
\begin{itemize}
\item Coverage and accuracy
\item Latency requirements
\item API availability
\item Cost vs. budget
\end{itemize}
\end{columns}
\bottomnote{Comparative analysis helps identify the right tool for specific requirements.}
\end{frame}

\begin{frame}[t]{Data Quality Frameworks}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Six Dimensions of Quality:}
\begin{enumerate}
\item \textbf{Accuracy:} Values correct?
\item \textbf{Completeness:} All required data present?
\item \textbf{Consistency:} No contradictions across sources?
\item \textbf{Timeliness:} Data current and available when needed?
\item \textbf{Validity:} Conforms to format, range, type?
\item \textbf{Uniqueness:} No duplicates?
\end{enumerate}

\vspace{0.5em}
\textbf{Measurement:}
\begin{itemize}
\item Accuracy: \% records passing validation
\item Completeness: \% non-null values
\item Consistency: \% cross-checks passing
\end{itemize}

\column{0.48\textwidth}
\textbf{Quality Assurance Process:}
\begin{enumerate}
\item Define quality rules
\item Automated checks (scripts)
\item Exception handling
\item Root cause analysis
\item Continuous monitoring
\item Feedback loop to data sources
\end{enumerate}

\vspace{0.5em}
\textbf{Tools:}
\begin{itemize}
\item Great Expectations (Python library)
\item dbt (data build tool)
\item Custom SQL checks
\item Dashboards (Tableau, Power BI)
\end{itemize}

\vspace{0.5em}
\textbf{Best Practice:} Shift-left testing (validate early in pipeline, not at model training).
\end{columns}
\bottomnote{Quality data is the foundation for effective machine learning models.}
\end{frame}

\begin{frame}[t]{Summary and Key Takeaways}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Data Types:}
\begin{itemize}
\item Structured (tables) vs. unstructured (text, images)
\item Traditional (prices, financials) vs. alternative (satellite, social)
\item Real-time (streaming) vs. batch (periodic)
\end{itemize}

\vspace{0.5em}
\textbf{Quality is Critical:}
\begin{itemize}
\item GIGO principle applies
\item 50-80\% of effort on data work
\item Bias detection and mitigation
\item Time series requires special care
\end{itemize}

\column{0.48\textwidth}
\textbf{Regulations Matter:}
\begin{itemize}
\item GDPR and privacy laws
\item Anonymization challenges
\item Right to explanation
\item Compliance > model performance
\end{itemize}

\vspace{0.5em}
\textbf{Practical Considerations:}
\begin{itemize}
\item Cost-benefit of data vendors
\item Infrastructure (cloud, storage)
\item Feature engineering creativity
\item Continuous monitoring
\end{itemize}
\end{columns}
\bottomnote{Financial data includes structured (prices), semi-structured (news), and unstructured (social media).}
\end{frame}

\begin{frame}[t]{Next Lesson Preview}
\textbf{Lesson 27: Supervised Learning - Regression}

\vspace{0.5em}
Topics to be covered:
\begin{itemize}
\item Features, labels, and training data
\item Simple and multiple linear regression
\item Coefficients interpretation
\item R-squared and evaluation metrics
\item Overfitting and regularization (Ridge, Lasso)
\item Applications: Stock return prediction, pricing models
\end{itemize}

\vspace{1em}
\textbf{Preparation:}
\begin{itemize}
\item Review basic linear algebra (vectors, matrices)
\item Recall correlation and covariance concepts
\item Think: What financial problems involve predicting continuous values?
\end{itemize}
\bottomnote{Linear regression provides interpretable baselines before trying complex models.}
\end{frame}

\end{document}
